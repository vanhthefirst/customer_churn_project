{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cells': [{'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['# Customer Churn Model Training\\n',\n",
       "    '\\n',\n",
       "    \"This notebook focuses on building and evaluating machine learning models to predict customer churn. We'll use the engineered features from our previous notebook to train various models and identify the most effective approach.\\n\",\n",
       "    '\\n',\n",
       "    '## Modeling Objectives\\n',\n",
       "    '\\n',\n",
       "    '1. **Data Preparation**: Prepare the feature-engineered data for modeling\\n',\n",
       "    '2. **Baseline Models**: Train simple models to establish a performance baseline\\n',\n",
       "    '3. **Advanced Models**: Implement more sophisticated algorithms with hyperparameter tuning\\n',\n",
       "    '4. **Model Evaluation**: Compare models using appropriate metrics\\n',\n",
       "    '5. **Feature Importance**: Identify the most important predictors of churn\\n',\n",
       "    '6. **Model Interpretation**: Understand how the model makes predictions']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Import libraries\\n',\n",
       "    'import pandas as pd\\n',\n",
       "    'import numpy as np\\n',\n",
       "    'import matplotlib.pyplot as plt\\n',\n",
       "    'import seaborn as sns\\n',\n",
       "    'from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\n',\n",
       "    'from sklearn.preprocessing import StandardScaler\\n',\n",
       "    'from sklearn.metrics import (\\n',\n",
       "    '    accuracy_score, precision_score, recall_score, f1_score,\\n',\n",
       "    '    roc_auc_score, confusion_matrix, classification_report,\\n',\n",
       "    '    precision_recall_curve, roc_curve, average_precision_score\\n',\n",
       "    ')\\n',\n",
       "    'from sklearn.linear_model import LogisticRegression\\n',\n",
       "    'from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n',\n",
       "    'from sklearn.svm import SVC\\n',\n",
       "    'from sklearn.neighbors import KNeighborsClassifier\\n',\n",
       "    'from sklearn.pipeline import Pipeline\\n',\n",
       "    'import pickle\\n',\n",
       "    'import os\\n',\n",
       "    '\\n',\n",
       "    '# Set plot style\\n',\n",
       "    \"plt.style.use('seaborn-whitegrid')\\n\",\n",
       "    \"sns.set_palette('colorblind')\\n\",\n",
       "    '%matplotlib inline']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Import our custom modules\\n',\n",
       "    'import sys\\n',\n",
       "    'import os\\n',\n",
       "    \"sys.path.append('..')\\n\",\n",
       "    'from src.model_trainer import (\\n',\n",
       "    '    split_data, train_logistic_regression, train_random_forest,\\n',\n",
       "    '    plot_feature_importance, save_model\\n',\n",
       "    ')']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 1. Load and Prepare Data']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Load the engineered dataset\\n',\n",
       "    \"df = pd.read_csv('../data/telco_churn_engineered.csv')\\n\",\n",
       "    'print(f\"Dataset shape: {df.shape}\")\\n',\n",
       "    '\\n',\n",
       "    '# Check a sample of the data\\n',\n",
       "    'df.head()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Preprocessing for modeling\\n',\n",
       "    '\\n',\n",
       "    '# Remove non-predictor columns\\n',\n",
       "    \"X = df.drop(['Churn', 'customerID'], axis=1)\\n\",\n",
       "    \"y = df['Churn']\\n\",\n",
       "    '\\n',\n",
       "    \"# Get categorical columns (except those we've already encoded)\\n\",\n",
       "    \"cat_cols = X.select_dtypes(include=['object']).columns\\n\",\n",
       "    'print(f\"Categorical columns: {len(cat_cols)}\")\\n',\n",
       "    'print(cat_cols.tolist())\\n',\n",
       "    '\\n',\n",
       "    '# One-hot encode categorical variables\\n',\n",
       "    'X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\\n',\n",
       "    'print(f\"Encoded feature shape: {X_encoded.shape}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Split the data into training and testing sets\\n',\n",
       "    'X_train, X_test, y_train, y_test = split_data(X_encoded, y, test_size=0.2, random_state=42)\\n',\n",
       "    '\\n',\n",
       "    '# Save column information for future prediction\\n',\n",
       "    \"pd.DataFrame(columns=X_train.columns).to_csv('../models/X_train_columns.csv', index=False)\\n\",\n",
       "    'print(\"Column information saved for future prediction\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 2. Baseline Models']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Function to evaluate model performance\\n',\n",
       "    'def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\"Model\"):\\n',\n",
       "    '    # Make predictions\\n',\n",
       "    '    y_train_pred = model.predict(X_train)\\n',\n",
       "    '    y_test_pred = model.predict(X_test)\\n',\n",
       "    '    \\n',\n",
       "    '    # Get probabilities for ROC\\n',\n",
       "    '    y_train_prob = model.predict_proba(X_train)[:, 1]\\n',\n",
       "    '    y_test_prob = model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '    \\n',\n",
       "    '    # Calculate metrics\\n',\n",
       "    '    train_accuracy = accuracy_score(y_train, y_train_pred)\\n',\n",
       "    '    test_accuracy = accuracy_score(y_test, y_test_pred)\\n',\n",
       "    '    \\n',\n",
       "    '    train_precision = precision_score(y_train, y_train_pred)\\n',\n",
       "    '    test_precision = precision_score(y_test, y_test_pred)\\n',\n",
       "    '    \\n',\n",
       "    '    train_recall = recall_score(y_train, y_train_pred)\\n',\n",
       "    '    test_recall = recall_score(y_test, y_test_pred)\\n',\n",
       "    '    \\n',\n",
       "    '    train_f1 = f1_score(y_train, y_train_pred)\\n',\n",
       "    '    test_f1 = f1_score(y_test, y_test_pred)\\n',\n",
       "    '    \\n',\n",
       "    '    train_auc = roc_auc_score(y_train, y_train_prob)\\n',\n",
       "    '    test_auc = roc_auc_score(y_test, y_test_prob)\\n',\n",
       "    '    \\n',\n",
       "    '    # Print results\\n',\n",
       "    '    print(f\"\\\\n{model_name} Performance:\")\\n',\n",
       "    '    print(f\"Training Accuracy: {train_accuracy:.4f}\")\\n',\n",
       "    '    print(f\"Testing Accuracy: {test_accuracy:.4f}\")\\n',\n",
       "    '    print(f\"Training AUC: {train_auc:.4f}\")\\n',\n",
       "    '    print(f\"Testing AUC: {test_auc:.4f}\")\\n',\n",
       "    '    print(f\"Training F1 Score: {train_f1:.4f}\")\\n',\n",
       "    '    print(f\"Testing F1 Score: {test_f1:.4f}\")\\n',\n",
       "    '    \\n',\n",
       "    '    # Confusion matrix for test set\\n',\n",
       "    '    cm = confusion_matrix(y_test, y_test_pred)\\n',\n",
       "    '    \\n',\n",
       "    '    # Classification report\\n',\n",
       "    '    print(f\"\\\\nClassification Report (Test Set):\")\\n',\n",
       "    '    print(classification_report(y_test, y_test_pred))\\n',\n",
       "    '    \\n',\n",
       "    '    # Visualize confusion matrix\\n',\n",
       "    '    plt.figure(figsize=(8, 6))\\n',\n",
       "    \"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
       "    \"                xticklabels=['Not Churn', 'Churn'],\\n\",\n",
       "    \"                yticklabels=['Not Churn', 'Churn'])\\n\",\n",
       "    '    plt.title(f\"{model_name} Confusion Matrix\")\\n',\n",
       "    \"    plt.ylabel('Actual')\\n\",\n",
       "    \"    plt.xlabel('Predicted')\\n\",\n",
       "    '    plt.show()\\n',\n",
       "    '    \\n',\n",
       "    '    # Return metrics dictionary\\n',\n",
       "    '    return {\\n',\n",
       "    \"        'model_name': model_name,\\n\",\n",
       "    \"        'train_accuracy': train_accuracy,\\n\",\n",
       "    \"        'test_accuracy': test_accuracy,\\n\",\n",
       "    \"        'train_precision': train_precision,\\n\",\n",
       "    \"        'test_precision': test_precision,\\n\",\n",
       "    \"        'train_recall': train_recall,\\n\",\n",
       "    \"        'test_recall': test_recall,\\n\",\n",
       "    \"        'train_f1': train_f1,\\n\",\n",
       "    \"        'test_f1': test_f1,\\n\",\n",
       "    \"        'train_auc': train_auc,\\n\",\n",
       "    \"        'test_auc': test_auc,\\n\",\n",
       "    \"        'confusion_matrix': cm\\n\",\n",
       "    '    }']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Train Logistic Regression baseline\\n',\n",
       "    'lr_model, _ = train_logistic_regression(X_train, y_train, X_test, y_test)\\n',\n",
       "    '\\n',\n",
       "    '# Evaluate logistic regression\\n',\n",
       "    'lr_metrics = evaluate_model(lr_model, X_train, X_test, y_train, y_test, \"Logistic Regression\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Logistic Regression with L1 regularization (feature selection)\\n',\n",
       "    \"lr_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)\\n\",\n",
       "    'lr_l1.fit(X_train, y_train)\\n',\n",
       "    'lr_l1_metrics = evaluate_model(lr_l1, X_train, X_test, y_train, y_test, \"Logistic Regression (L1)\")\\n',\n",
       "    '\\n',\n",
       "    '# Analyze logistic regression coefficients\\n',\n",
       "    'coef = pd.DataFrame({\\n',\n",
       "    \"    'Feature': X_train.columns,\\n\",\n",
       "    \"    'Coefficient': lr_l1.coef_[0]\\n\",\n",
       "    '})\\n',\n",
       "    '\\n',\n",
       "    '# Show non-zero coefficients (selected features)\\n',\n",
       "    \"non_zero_coef = coef[coef['Coefficient'] != 0].sort_values('Coefficient', ascending=False)\\n\",\n",
       "    'print(f\"\\\\nNumber of features selected by L1 regularization: {len(non_zero_coef)}\")\\n',\n",
       "    '\\n',\n",
       "    '# Visualize top coefficients\\n',\n",
       "    'plt.figure(figsize=(12, 8))\\n',\n",
       "    'top_coef = non_zero_coef.head(15)\\n',\n",
       "    'bottom_coef = non_zero_coef.tail(15)\\n',\n",
       "    'coef_to_plot = pd.concat([top_coef, bottom_coef])\\n',\n",
       "    '\\n',\n",
       "    '# Sort for visualization\\n',\n",
       "    \"coef_to_plot = coef_to_plot.sort_values('Coefficient')\\n\",\n",
       "    '\\n',\n",
       "    '# Plot\\n',\n",
       "    \"colors = ['red' if x < 0 else 'green' for x in coef_to_plot['Coefficient']]\\n\",\n",
       "    \"plt.barh(coef_to_plot['Feature'], coef_to_plot['Coefficient'], color=colors)\\n\",\n",
       "    \"plt.title('Top Logistic Regression Coefficients (L1 Regularization)')\\n\",\n",
       "    \"plt.xlabel('Coefficient Value')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Train Random Forest baseline\\n',\n",
       "    'rf_model, _ = train_random_forest(X_train, y_train, X_test, y_test)\\n',\n",
       "    '\\n',\n",
       "    '# Evaluate Random Forest\\n',\n",
       "    'rf_metrics = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \"Random Forest\")\\n',\n",
       "    '\\n',\n",
       "    '# Plot feature importance\\n',\n",
       "    'plt, feature_importances = plot_feature_importance(rf_model, X_train, n_features=20)\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 3. Advanced Models']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Gradient Boosting Classifier\\n',\n",
       "    'gb_model = GradientBoostingClassifier(random_state=42)\\n',\n",
       "    'gb_model.fit(X_train, y_train)\\n',\n",
       "    'gb_metrics = evaluate_model(gb_model, X_train, X_test, y_train, y_test, \"Gradient Boosting\")\\n',\n",
       "    '\\n',\n",
       "    '# Gradient Boosting feature importance\\n',\n",
       "    'gb_feature_importances = pd.DataFrame({\\n',\n",
       "    \"    'Feature': X_train.columns,\\n\",\n",
       "    \"    'Importance': gb_model.feature_importances_\\n\",\n",
       "    \"}).sort_values('Importance', ascending=False)\\n\",\n",
       "    '\\n',\n",
       "    'plt.figure(figsize=(12, 8))\\n',\n",
       "    \"sns.barplot(x='Importance', y='Feature', data=gb_feature_importances.head(20))\\n\",\n",
       "    \"plt.title('Top 20 Features (Gradient Boosting)')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Hyperparameter tuning for Gradient Boosting\\n',\n",
       "    'param_grid = {\\n',\n",
       "    \"    'n_estimators': [100, 200],\\n\",\n",
       "    \"    'learning_rate': [0.05, 0.1],\\n\",\n",
       "    \"    'max_depth': [3, 5],\\n\",\n",
       "    \"    'min_samples_split': [2, 5],\\n\",\n",
       "    \"    'min_samples_leaf': [1, 2]\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    '# Comment out for faster execution\\n',\n",
       "    'gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), \\n',\n",
       "    \"                      param_grid, cv=5, scoring='roc_auc')\\n\",\n",
       "    'gb_grid.fit(X_train, y_train)\\n',\n",
       "    '\\n',\n",
       "    'print(f\"Best parameters: {gb_grid.best_params_}\")\\n',\n",
       "    'print(f\"Best CV score: {gb_grid.best_score_:.4f}\")\\n',\n",
       "    '\\n',\n",
       "    '# Evaluate tuned model\\n',\n",
       "    'gb_tuned = gb_grid.best_estimator_\\n',\n",
       "    'gb_tuned_metrics = evaluate_model(gb_tuned, X_train, X_test, y_train, y_test, \"Tuned Gradient Boosting\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 4. Model Comparison']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Collect metrics for all models\\n',\n",
       "    'models_metrics = [lr_metrics, lr_l1_metrics, rf_metrics, gb_metrics, gb_tuned_metrics]\\n',\n",
       "    \"models_names = [metric['model_name'] for metric in models_metrics]\\n\",\n",
       "    '\\n',\n",
       "    '# Create comparison dataframe\\n',\n",
       "    \"comparison_metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_auc']\\n\",\n",
       "    'comparison_df = pd.DataFrame()\\n',\n",
       "    '\\n',\n",
       "    'for metric in comparison_metrics:\\n',\n",
       "    '    comparison_df[metric] = [m[metric] for m in models_metrics]\\n',\n",
       "    '\\n',\n",
       "    'comparison_df.index = models_names\\n',\n",
       "    '\\n',\n",
       "    '# Rename columns for better display\\n',\n",
       "    \"comparison_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\\n\",\n",
       "    '\\n',\n",
       "    '# Display comparison\\n',\n",
       "    'print(\"Model Performance Comparison (Test Set):\")\\n',\n",
       "    'comparison_df']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Visualize model comparison\\n',\n",
       "    'plt.figure(figsize=(12, 8))\\n',\n",
       "    \"comparison_df.plot(kind='bar', figsize=(12, 8))\\n\",\n",
       "    \"plt.title('Model Performance Comparison')\\n\",\n",
       "    \"plt.ylabel('Score')\\n\",\n",
       "    'plt.ylim(0, 1)\\n',\n",
       "    'plt.xticks(rotation=45)\\n',\n",
       "    \"plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# ROC curve comparison\\n',\n",
       "    'plt.figure(figsize=(10, 8))\\n',\n",
       "    '\\n',\n",
       "    '# Models to include in comparison\\n',\n",
       "    'models = [\\n',\n",
       "    '    (lr_model, \"Logistic Regression\", \\'blue\\'),\\n',\n",
       "    '    (rf_model, \"Random Forest\", \\'green\\'),\\n',\n",
       "    '    (gb_tuned, \"Tuned Gradient Boosting\", \\'red\\')\\n',\n",
       "    ']\\n',\n",
       "    '\\n',\n",
       "    'for model, name, color in models:\\n',\n",
       "    '    y_prob = model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '    fpr, tpr, _ = roc_curve(y_test, y_prob)\\n',\n",
       "    '    auc = roc_auc_score(y_test, y_prob)\\n',\n",
       "    \"    plt.plot(fpr, tpr, color=color, label=f'{name} (AUC = {auc:.3f})')\\n\",\n",
       "    '\\n',\n",
       "    '# Add diagonal line (random classifier)\\n',\n",
       "    \"plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\\n\",\n",
       "    '\\n',\n",
       "    \"plt.xlabel('False Positive Rate')\\n\",\n",
       "    \"plt.ylabel('True Positive Rate')\\n\",\n",
       "    \"plt.title('ROC Curve Comparison')\\n\",\n",
       "    \"plt.legend(loc='lower right')\\n\",\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Precision-Recall curve comparison\\n',\n",
       "    'plt.figure(figsize=(10, 8))\\n',\n",
       "    '\\n',\n",
       "    'for model, name, color in models:\\n',\n",
       "    '    y_prob = model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '    precision, recall, _ = precision_recall_curve(y_test, y_prob)\\n',\n",
       "    '    ap = average_precision_score(y_test, y_prob)\\n',\n",
       "    \"    plt.plot(recall, precision, color=color, label=f'{name} (AP = {ap:.3f})')\\n\",\n",
       "    '\\n',\n",
       "    \"plt.xlabel('Recall')\\n\",\n",
       "    \"plt.ylabel('Precision')\\n\",\n",
       "    \"plt.title('Precision-Recall Curve Comparison')\\n\",\n",
       "    \"plt.legend(loc='lower left')\\n\",\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 5. Feature Importance Analysis']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Feature importance comparison between Random Forest and Gradient Boosting\\n',\n",
       "    '# Extract feature importances\\n',\n",
       "    'rf_importance = pd.DataFrame({\\n',\n",
       "    \"    'Feature': X_train.columns,\\n\",\n",
       "    \"    'RF_Importance': rf_model.feature_importances_\\n\",\n",
       "    '})\\n',\n",
       "    '\\n',\n",
       "    'gb_importance = pd.DataFrame({\\n',\n",
       "    \"    'Feature': X_train.columns,\\n\",\n",
       "    \"    'GB_Importance': gb_tuned.feature_importances_\\n\",\n",
       "    '})\\n',\n",
       "    '\\n',\n",
       "    '# Merge importances\\n',\n",
       "    \"feature_comparison = pd.merge(rf_importance, gb_importance, on='Feature')\\n\",\n",
       "    '\\n',\n",
       "    '# Add logistic regression coefficients\\n',\n",
       "    'lr_coef = pd.DataFrame({\\n',\n",
       "    \"    'Feature': X_train.columns,\\n\",\n",
       "    \"    'LR_Coefficient': np.abs(lr_model.coef_[0])  # Take absolute value for comparison\\n\",\n",
       "    '})\\n',\n",
       "    '\\n',\n",
       "    \"feature_comparison = pd.merge(feature_comparison, lr_coef, on='Feature')\\n\",\n",
       "    '\\n',\n",
       "    '# Normalize coefficients to make them comparable\\n',\n",
       "    \"feature_comparison['LR_Importance'] = feature_comparison['LR_Coefficient'] / feature_comparison['LR_Coefficient'].max()\\n\",\n",
       "    '\\n',\n",
       "    '# Calculate average importance\\n',\n",
       "    \"feature_comparison['Avg_Importance'] = (\\n\",\n",
       "    \"    feature_comparison['RF_Importance'] + \\n\",\n",
       "    \"    feature_comparison['GB_Importance'] + \\n\",\n",
       "    \"    feature_comparison['LR_Importance']\\n\",\n",
       "    ') / 3\\n',\n",
       "    '\\n',\n",
       "    '# Sort by average importance\\n',\n",
       "    \"feature_comparison = feature_comparison.sort_values('Avg_Importance', ascending=False)\\n\",\n",
       "    '\\n',\n",
       "    '# Display top features\\n',\n",
       "    'print(\"Top 15 Features by Average Importance:\")\\n',\n",
       "    \"feature_comparison[['Feature', 'RF_Importance', 'GB_Importance', 'LR_Importance', 'Avg_Importance']].head(15)\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Visualize top feature importance comparison\\n',\n",
       "    \"top_features = feature_comparison.head(15)['Feature'].tolist()\\n\",\n",
       "    'importance_df = pd.DataFrame()\\n',\n",
       "    '\\n',\n",
       "    \"for model, name in [(rf_model, 'Random Forest'), (gb_tuned, 'Gradient Boosting')]:\\n\",\n",
       "    '    imp = pd.DataFrame({\\n',\n",
       "    \"        'Feature': X_train.columns,\\n\",\n",
       "    \"        'Importance': model.feature_importances_,\\n\",\n",
       "    \"        'Model': name\\n\",\n",
       "    '    })\\n',\n",
       "    '    importance_df = pd.concat([importance_df, imp])\\n',\n",
       "    '\\n',\n",
       "    '# Filter for top features\\n',\n",
       "    \"top_importance = importance_df[importance_df['Feature'].isin(top_features)]\\n\",\n",
       "    '\\n',\n",
       "    '# Create barplot\\n',\n",
       "    'plt.figure(figsize=(12, 10))\\n',\n",
       "    \"sns.barplot(x='Importance', y='Feature', hue='Model', data=top_importance)\\n\",\n",
       "    \"plt.title('Top 15 Feature Importance Comparison')\\n\",\n",
       "    \"plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\n\",\n",
       "    'plt.tight_layout()\\n',\n",
       "    'plt.show()']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 6. Final Model Selection']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Based on evaluation metrics, select the best model\\n',\n",
       "    '# (In this case, Gradient Boosting typically performs best)\\n',\n",
       "    'best_model = gb_tuned\\n',\n",
       "    '\\n',\n",
       "    '# Print best model parameters\\n',\n",
       "    'print(\"Best Model: Tuned Gradient Boosting\")\\n',\n",
       "    'print(f\"Parameters: {best_model.get_params()}\")\\n',\n",
       "    '\\n',\n",
       "    '# Save the best model\\n',\n",
       "    \"os.makedirs('../models', exist_ok=True)\\n\",\n",
       "    \"save_model(best_model, '../models/churn_model.pkl')\"]},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Analyze misclassifications\\n',\n",
       "    'y_pred = best_model.predict(X_test)\\n',\n",
       "    'y_prob = best_model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '\\n',\n",
       "    '# Create dataframe with actual and predicted values\\n',\n",
       "    'misclass_df = pd.DataFrame({\\n',\n",
       "    \"    'actual': y_test,\\n\",\n",
       "    \"    'predicted': y_pred,\\n\",\n",
       "    \"    'probability': y_prob\\n\",\n",
       "    '})\\n',\n",
       "    '\\n',\n",
       "    '# Add original features\\n',\n",
       "    'misclass_df = pd.concat([X_test.reset_index(drop=True), misclass_df], axis=1)\\n',\n",
       "    '\\n',\n",
       "    \"# False positives (predicted churn but didn't)\\n\",\n",
       "    \"false_positives = misclass_df[(misclass_df['actual'] == 0) & (misclass_df['predicted'] == 1)]\\n\",\n",
       "    'print(f\"Number of False Positives: {len(false_positives)}\")\\n',\n",
       "    '\\n',\n",
       "    \"# False negatives (didn't predict churn but did)\\n\",\n",
       "    \"false_negatives = misclass_df[(misclass_df['actual'] == 1) & (misclass_df['predicted'] == 0)]\\n\",\n",
       "    'print(f\"Number of False Negatives: {len(false_negatives)}\")\\n',\n",
       "    '\\n',\n",
       "    '# Analyze false negatives (most costly error type)\\n',\n",
       "    'print(\"\\\\nFalse Negative Analysis:\")\\n',\n",
       "    \"fn_numerical = false_negatives.select_dtypes(include=['int64', 'float64'])\\n\",\n",
       "    'fn_means = fn_numerical.mean()\\n',\n",
       "    '\\n',\n",
       "    '# Compare to overall population means\\n',\n",
       "    \"overall_means = X_test.select_dtypes(include=['int64', 'float64']).mean()\\n\",\n",
       "    'percent_diff = ((fn_means - overall_means) / overall_means * 100).dropna()\\n',\n",
       "    '\\n',\n",
       "    '# Show features with large differences\\n',\n",
       "    'significant_diffs = percent_diff[abs(percent_diff) > 20].sort_values(ascending=False)\\n',\n",
       "    'print(\"\\\\nFeatures where False Negatives differ from population:\")\\n',\n",
       "    'for feature, diff in significant_diffs.items():\\n',\n",
       "    '    direction = \"higher\" if diff > 0 else \"lower\"\\n',\n",
       "    '    print(f\"{feature}: {abs(diff):.1f}% {direction}\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## 7. Threshold Optimization']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Threshold optimization\\n',\n",
       "    '# Instead of using default 0.5 threshold, we can optimize it for specific business objectives\\n',\n",
       "    '\\n',\n",
       "    '# Get probabilities\\n',\n",
       "    'y_prob = best_model.predict_proba(X_test)[:, 1]\\n',\n",
       "    '\\n',\n",
       "    '# Calculate metrics across different thresholds\\n',\n",
       "    'thresholds = np.arange(0.1, 0.9, 0.05)\\n',\n",
       "    'threshold_metrics = []\\n',\n",
       "    '\\n',\n",
       "    'for threshold in thresholds:\\n',\n",
       "    '    y_pred_threshold = (y_prob >= threshold).astype(int)\\n',\n",
       "    '    \\n',\n",
       "    '    # Calculate metrics\\n',\n",
       "    '    accuracy = accuracy_score(y_test, y_pred_threshold)\\n',\n",
       "    '    precision = precision_score(y_test, y_pred_threshold)\\n',\n",
       "    '    recall = recall_score(y_test, y_pred_threshold)\\n',\n",
       "    '    f1 = f1_score(y_test, y_pred_threshold)\\n',\n",
       "    '    \\n',\n",
       "    '    # Confusion matrix\\n',\n",
       "    '    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\\n',\n",
       "    '    \\n',\n",
       "    '    # Assuming cost of false negative is 5x cost of false positive\\n',\n",
       "    '    # This is a business assumption - adjust based on your specific situation\\n',\n",
       "    '    cost = (fn * 5) + fp\\n',\n",
       "    '    \\n',\n",
       "    '    threshold_metrics.append({\\n',\n",
       "    \"        'threshold': threshold,\\n\",\n",
       "    \"        'accuracy': accuracy,\\n\",\n",
       "    \"        'precision': precision,\\n\",\n",
       "    \"        'recall': recall,\\n\",\n",
       "    \"        'f1': f1,\\n\",\n",
       "    \"        'tp': tp,\\n\",\n",
       "    \"        'fp': fp,\\n\",\n",
       "    \"        'tn': tn,\\n\",\n",
       "    \"        'fn': fn,\\n\",\n",
       "    \"        'cost': cost\\n\",\n",
       "    '    })\\n',\n",
       "    '\\n',\n",
       "    '# Convert to DataFrame\\n',\n",
       "    'threshold_df = pd.DataFrame(threshold_metrics)\\n',\n",
       "    '\\n',\n",
       "    '# Plot metrics vs threshold\\n',\n",
       "    'plt.figure(figsize=(12, 8))\\n',\n",
       "    \"plt.plot(threshold_df['threshold'], threshold_df['accuracy'], label='Accuracy')\\n\",\n",
       "    \"plt.plot(threshold_df['threshold'], threshold_df['precision'], label='Precision')\\n\",\n",
       "    \"plt.plot(threshold_df['threshold'], threshold_df['recall'], label='Recall')\\n\",\n",
       "    \"plt.plot(threshold_df['threshold'], threshold_df['f1'], label='F1 Score')\\n\",\n",
       "    \"plt.xlabel('Threshold')\\n\",\n",
       "    \"plt.ylabel('Score')\\n\",\n",
       "    \"plt.title('Metrics vs. Classification Threshold')\\n\",\n",
       "    'plt.grid(True)\\n',\n",
       "    'plt.legend()\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Plot cost vs threshold\\n',\n",
       "    'plt.figure(figsize=(10, 6))\\n',\n",
       "    \"plt.plot(threshold_df['threshold'], threshold_df['cost'])\\n\",\n",
       "    \"plt.xlabel('Threshold')\\n\",\n",
       "    \"plt.ylabel('Cost (5×FN + FP)')\\n\",\n",
       "    \"plt.title('Error Cost vs. Threshold')\\n\",\n",
       "    'plt.grid(True)\\n',\n",
       "    'plt.show()\\n',\n",
       "    '\\n',\n",
       "    '# Find optimal threshold for minimizing cost\\n',\n",
       "    \"optimal_idx = threshold_df['cost'].idxmin()\\n\",\n",
       "    \"optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\\n\",\n",
       "    'print(f\"Optimal threshold for minimizing cost: {optimal_threshold:.2f}\")\\n',\n",
       "    'print(\"Metrics at optimal threshold:\")\\n',\n",
       "    'print(f\"Accuracy: {threshold_df.loc[optimal_idx, \\'accuracy\\']:.4f}\")\\n',\n",
       "    'print(f\"Precision: {threshold_df.loc[optimal_idx, \\'precision\\']:.4f}\")\\n',\n",
       "    'print(f\"Recall: {threshold_df.loc[optimal_idx, \\'recall\\']:.4f}\")\\n',\n",
       "    'print(f\"F1 Score: {threshold_df.loc[optimal_idx, \\'f1\\']:.4f}\")\\n',\n",
       "    'print(f\"False Negatives: {threshold_df.loc[optimal_idx, \\'fn\\']}\")\\n',\n",
       "    'print(f\"False Positives: {threshold_df.loc[optimal_idx, \\'fp\\']}\")']},\n",
       "  {'cell_type': 'code',\n",
       "   'execution_count': None,\n",
       "   'metadata': {},\n",
       "   'source': ['# Save the optimal threshold with the model information\\n',\n",
       "    'model_info = {\\n',\n",
       "    \"    'model': best_model,\\n\",\n",
       "    \"    'optimal_threshold': optimal_threshold,\\n\",\n",
       "    \"    'feature_importance': feature_comparison[['Feature', 'Avg_Importance']].head(20).to_dict(),\\n\",\n",
       "    \"    'columns': X_train.columns.tolist()\\n\",\n",
       "    '}\\n',\n",
       "    '\\n',\n",
       "    \"with open('../models/model_info.pkl', 'wb') as f:\\n\",\n",
       "    '    pickle.dump(model_info, f)\\n',\n",
       "    '\\n',\n",
       "    'print(\"Model information saved with optimal threshold\")']},\n",
       "  {'cell_type': 'markdown',\n",
       "   'metadata': {},\n",
       "   'source': ['## Model Summary\\n',\n",
       "    '\\n',\n",
       "    \"In this notebook, we've developed and evaluated several machine learning models to predict customer churn. Here's a summary of our findings:\\n\",\n",
       "    '\\n',\n",
       "    '### Model Performance\\n',\n",
       "    '\\n',\n",
       "    '1. **Gradient Boosting** emerged as the best performing model with the highest AUC and F1 scores after hyperparameter tuning.\\n',\n",
       "    '2. **Random Forest** also performed well and provided valuable insights through feature importance.\\n',\n",
       "    '3. **Logistic Regression** offered good interpretability and highlighted key linear relationships.\\n',\n",
       "    '\\n',\n",
       "    '### Key Predictors of Churn\\n',\n",
       "    '\\n',\n",
       "    'The most important features across models were:\\n',\n",
       "    '\\n',\n",
       "    '1. **Contract-Related Features**: `ContractRiskFactor`, `Contract_Month-to-month`, and contract-related variables consistently ranked as top predictors.\\n',\n",
       "    '2. **Tenure**: Tenure-related features like `tenure` and `TenureGroup` were strong negative predictors of churn.\\n',\n",
       "    '3. **Service Usage**: Features like `TotalServices`, `FiberNoProtection`, and specific service flags were important.\\n',\n",
       "    '4. **Composite Features**: The engineered `CompositeRiskScore` captured multiple risk factors effectively.\\n',\n",
       "    '5. **Customer Value**: Features like `MonthlyCharges` and the interaction between high value and short contracts were significant.\\n',\n",
       "    '\\n',\n",
       "    '### Threshold Optimization\\n',\n",
       "    '\\n',\n",
       "    'We optimized the classification threshold to minimize the cost of misclassifications, considering that false negatives (failing to predict customers who will churn) are more costly than false positives. The optimal threshold was determined to be around 0.3 (instead of the default 0.5), which improves recall at the expense of some precision.\\n',\n",
       "    '\\n',\n",
       "    '### Next Steps\\n',\n",
       "    '\\n',\n",
       "    '1. **Business Analysis**: Apply the model to segment customers by churn risk and develop targeted retention strategies.\\n',\n",
       "    '2. **Deployment**: Implement the model in a production system for ongoing churn prediction.\\n',\n",
       "    '3. **Monitoring**: Establish metrics to monitor model performance over time and retrain as needed.\\n',\n",
       "    '4. **Feedback Loop**: Incorporate the effectiveness of retention actions back into the model to improve future predictions.']}],\n",
       " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "   'language': 'python',\n",
       "   'name': 'python3'},\n",
       "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "   'file_extension': '.py',\n",
       "   'mimetype': 'text/x-python',\n",
       "   'name': 'python',\n",
       "   'nbconvert_exporter': 'python',\n",
       "   'pygments_lexer': 'ipython3',\n",
       "   'version': '3.8.10'}},\n",
       " 'nbformat': 4,\n",
       " 'nbformat_minor': 4}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Customer Churn Model Training\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook focuses on building and evaluating machine learning models to predict customer churn. We'll use the engineered features from our previous notebook to train various models and identify the most effective approach.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Modeling Objectives\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Data Preparation**: Prepare the feature-engineered data for modeling\\n\",\n",
    "    \"2. **Baseline Models**: Train simple models to establish a performance baseline\\n\",\n",
    "    \"3. **Advanced Models**: Implement more sophisticated algorithms with hyperparameter tuning\\n\",\n",
    "    \"4. **Model Evaluation**: Compare models using appropriate metrics\\n\",\n",
    "    \"5. **Feature Importance**: Identify the most important predictors of churn\\n\",\n",
    "    \"6. **Model Interpretation**: Understand how the model makes predictions\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"from sklearn.metrics import (\\n\",\n",
    "    \"    accuracy_score, precision_score, recall_score, f1_score,\\n\",\n",
    "    \"    roc_auc_score, confusion_matrix, classification_report,\\n\",\n",
    "    \"    precision_recall_curve, roc_curve, average_precision_score\\n\",\n",
    "    \")\\n\",\n",
    "    \"from sklearn.linear_model import LogisticRegression\\n\",\n",
    "    \"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\\n\",\n",
    "    \"from sklearn.svm import SVC\\n\",\n",
    "    \"from sklearn.neighbors import KNeighborsClassifier\\n\",\n",
    "    \"from sklearn.pipeline import Pipeline\\n\",\n",
    "    \"import pickle\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plot style\\n\",\n",
    "    \"plt.style.use('seaborn-whitegrid')\\n\",\n",
    "    \"sns.set_palette('colorblind')\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Import our custom modules\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"from src.model_trainer import (\\n\",\n",
    "    \"    split_data, train_logistic_regression, train_random_forest,\\n\",\n",
    "    \"    plot_feature_importance, save_model\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Load and Prepare Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Load the engineered dataset\\n\",\n",
    "    \"df = pd.read_csv('../data/telco_churn_engineered.csv')\\n\",\n",
    "    \"print(f\\\"Dataset shape: {df.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Check a sample of the data\\n\",\n",
    "    \"df.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Preprocessing for modeling\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Remove non-predictor columns\\n\",\n",
    "    \"X = df.drop(['Churn', 'customerID'], axis=1)\\n\",\n",
    "    \"y = df['Churn']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get categorical columns (except those we've already encoded)\\n\",\n",
    "    \"cat_cols = X.select_dtypes(include=['object']).columns\\n\",\n",
    "    \"print(f\\\"Categorical columns: {len(cat_cols)}\\\")\\n\",\n",
    "    \"print(cat_cols.tolist())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# One-hot encode categorical variables\\n\",\n",
    "    \"X_encoded = pd.get_dummies(X, columns=cat_cols, drop_first=True)\\n\",\n",
    "    \"print(f\\\"Encoded feature shape: {X_encoded.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Split the data into training and testing sets\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = split_data(X_encoded, y, test_size=0.2, random_state=42)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save column information for future prediction\\n\",\n",
    "    \"pd.DataFrame(columns=X_train.columns).to_csv('../models/X_train_columns.csv', index=False)\\n\",\n",
    "    \"print(\\\"Column information saved for future prediction\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Baseline Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Function to evaluate model performance\\n\",\n",
    "    \"def evaluate_model(model, X_train, X_test, y_train, y_test, model_name=\\\"Model\\\"):\\n\",\n",
    "    \"    # Make predictions\\n\",\n",
    "    \"    y_train_pred = model.predict(X_train)\\n\",\n",
    "    \"    y_test_pred = model.predict(X_test)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Get probabilities for ROC\\n\",\n",
    "    \"    y_train_prob = model.predict_proba(X_train)[:, 1]\\n\",\n",
    "    \"    y_test_prob = model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate metrics\\n\",\n",
    "    \"    train_accuracy = accuracy_score(y_train, y_train_pred)\\n\",\n",
    "    \"    test_accuracy = accuracy_score(y_test, y_test_pred)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    train_precision = precision_score(y_train, y_train_pred)\\n\",\n",
    "    \"    test_precision = precision_score(y_test, y_test_pred)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    train_recall = recall_score(y_train, y_train_pred)\\n\",\n",
    "    \"    test_recall = recall_score(y_test, y_test_pred)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    train_f1 = f1_score(y_train, y_train_pred)\\n\",\n",
    "    \"    test_f1 = f1_score(y_test, y_test_pred)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    train_auc = roc_auc_score(y_train, y_train_prob)\\n\",\n",
    "    \"    test_auc = roc_auc_score(y_test, y_test_prob)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Print results\\n\",\n",
    "    \"    print(f\\\"\\\\n{model_name} Performance:\\\")\\n\",\n",
    "    \"    print(f\\\"Training Accuracy: {train_accuracy:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Testing Accuracy: {test_accuracy:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Training AUC: {train_auc:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Testing AUC: {test_auc:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Training F1 Score: {train_f1:.4f}\\\")\\n\",\n",
    "    \"    print(f\\\"Testing F1 Score: {test_f1:.4f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Confusion matrix for test set\\n\",\n",
    "    \"    cm = confusion_matrix(y_test, y_test_pred)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Classification report\\n\",\n",
    "    \"    print(f\\\"\\\\nClassification Report (Test Set):\\\")\\n\",\n",
    "    \"    print(classification_report(y_test, y_test_pred))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Visualize confusion matrix\\n\",\n",
    "    \"    plt.figure(figsize=(8, 6))\\n\",\n",
    "    \"    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \\n\",\n",
    "    \"                xticklabels=['Not Churn', 'Churn'],\\n\",\n",
    "    \"                yticklabels=['Not Churn', 'Churn'])\\n\",\n",
    "    \"    plt.title(f\\\"{model_name} Confusion Matrix\\\")\\n\",\n",
    "    \"    plt.ylabel('Actual')\\n\",\n",
    "    \"    plt.xlabel('Predicted')\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Return metrics dictionary\\n\",\n",
    "    \"    return {\\n\",\n",
    "    \"        'model_name': model_name,\\n\",\n",
    "    \"        'train_accuracy': train_accuracy,\\n\",\n",
    "    \"        'test_accuracy': test_accuracy,\\n\",\n",
    "    \"        'train_precision': train_precision,\\n\",\n",
    "    \"        'test_precision': test_precision,\\n\",\n",
    "    \"        'train_recall': train_recall,\\n\",\n",
    "    \"        'test_recall': test_recall,\\n\",\n",
    "    \"        'train_f1': train_f1,\\n\",\n",
    "    \"        'test_f1': test_f1,\\n\",\n",
    "    \"        'train_auc': train_auc,\\n\",\n",
    "    \"        'test_auc': test_auc,\\n\",\n",
    "    \"        'confusion_matrix': cm\\n\",\n",
    "    \"    }\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Train Logistic Regression baseline\\n\",\n",
    "    \"lr_model, _ = train_logistic_regression(X_train, y_train, X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate logistic regression\\n\",\n",
    "    \"lr_metrics = evaluate_model(lr_model, X_train, X_test, y_train, y_test, \\\"Logistic Regression\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Logistic Regression with L1 regularization (feature selection)\\n\",\n",
    "    \"lr_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=0.1, random_state=42)\\n\",\n",
    "    \"lr_l1.fit(X_train, y_train)\\n\",\n",
    "    \"lr_l1_metrics = evaluate_model(lr_l1, X_train, X_test, y_train, y_test, \\\"Logistic Regression (L1)\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze logistic regression coefficients\\n\",\n",
    "    \"coef = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_train.columns,\\n\",\n",
    "    \"    'Coefficient': lr_l1.coef_[0]\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show non-zero coefficients (selected features)\\n\",\n",
    "    \"non_zero_coef = coef[coef['Coefficient'] != 0].sort_values('Coefficient', ascending=False)\\n\",\n",
    "    \"print(f\\\"\\\\nNumber of features selected by L1 regularization: {len(non_zero_coef)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Visualize top coefficients\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"top_coef = non_zero_coef.head(15)\\n\",\n",
    "    \"bottom_coef = non_zero_coef.tail(15)\\n\",\n",
    "    \"coef_to_plot = pd.concat([top_coef, bottom_coef])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort for visualization\\n\",\n",
    "    \"coef_to_plot = coef_to_plot.sort_values('Coefficient')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot\\n\",\n",
    "    \"colors = ['red' if x < 0 else 'green' for x in coef_to_plot['Coefficient']]\\n\",\n",
    "    \"plt.barh(coef_to_plot['Feature'], coef_to_plot['Coefficient'], color=colors)\\n\",\n",
    "    \"plt.title('Top Logistic Regression Coefficients (L1 Regularization)')\\n\",\n",
    "    \"plt.xlabel('Coefficient Value')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Train Random Forest baseline\\n\",\n",
    "    \"rf_model, _ = train_random_forest(X_train, y_train, X_test, y_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate Random Forest\\n\",\n",
    "    \"rf_metrics = evaluate_model(rf_model, X_train, X_test, y_train, y_test, \\\"Random Forest\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot feature importance\\n\",\n",
    "    \"plt, feature_importances = plot_feature_importance(rf_model, X_train, n_features=20)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Advanced Models\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Gradient Boosting Classifier\\n\",\n",
    "    \"gb_model = GradientBoostingClassifier(random_state=42)\\n\",\n",
    "    \"gb_model.fit(X_train, y_train)\\n\",\n",
    "    \"gb_metrics = evaluate_model(gb_model, X_train, X_test, y_train, y_test, \\\"Gradient Boosting\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Gradient Boosting feature importance\\n\",\n",
    "    \"gb_feature_importances = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_train.columns,\\n\",\n",
    "    \"    'Importance': gb_model.feature_importances_\\n\",\n",
    "    \"}).sort_values('Importance', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"sns.barplot(x='Importance', y='Feature', data=gb_feature_importances.head(20))\\n\",\n",
    "    \"plt.title('Top 20 Features (Gradient Boosting)')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Hyperparameter tuning for Gradient Boosting\\n\",\n",
    "    \"param_grid = {\\n\",\n",
    "    \"    'n_estimators': [100, 200],\\n\",\n",
    "    \"    'learning_rate': [0.05, 0.1],\\n\",\n",
    "    \"    'max_depth': [3, 5],\\n\",\n",
    "    \"    'min_samples_split': [2, 5],\\n\",\n",
    "    \"    'min_samples_leaf': [1, 2]\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Comment out for faster execution\\n\",\n",
    "    \"gb_grid = GridSearchCV(GradientBoostingClassifier(random_state=42), \\n\",\n",
    "    \"                      param_grid, cv=5, scoring='roc_auc')\\n\",\n",
    "    \"gb_grid.fit(X_train, y_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Best parameters: {gb_grid.best_params_}\\\")\\n\",\n",
    "    \"print(f\\\"Best CV score: {gb_grid.best_score_:.4f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Evaluate tuned model\\n\",\n",
    "    \"gb_tuned = gb_grid.best_estimator_\\n\",\n",
    "    \"gb_tuned_metrics = evaluate_model(gb_tuned, X_train, X_test, y_train, y_test, \\\"Tuned Gradient Boosting\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Model Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Collect metrics for all models\\n\",\n",
    "    \"models_metrics = [lr_metrics, lr_l1_metrics, rf_metrics, gb_metrics, gb_tuned_metrics]\\n\",\n",
    "    \"models_names = [metric['model_name'] for metric in models_metrics]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create comparison dataframe\\n\",\n",
    "    \"comparison_metrics = ['test_accuracy', 'test_precision', 'test_recall', 'test_f1', 'test_auc']\\n\",\n",
    "    \"comparison_df = pd.DataFrame()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for metric in comparison_metrics:\\n\",\n",
    "    \"    comparison_df[metric] = [m[metric] for m in models_metrics]\\n\",\n",
    "    \"\\n\",\n",
    "    \"comparison_df.index = models_names\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Rename columns for better display\\n\",\n",
    "    \"comparison_df.columns = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'AUC']\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display comparison\\n\",\n",
    "    \"print(\\\"Model Performance Comparison (Test Set):\\\")\\n\",\n",
    "    \"comparison_df\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Visualize model comparison\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"comparison_df.plot(kind='bar', figsize=(12, 8))\\n\",\n",
    "    \"plt.title('Model Performance Comparison')\\n\",\n",
    "    \"plt.ylabel('Score')\\n\",\n",
    "    \"plt.ylim(0, 1)\\n\",\n",
    "    \"plt.xticks(rotation=45)\\n\",\n",
    "    \"plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# ROC curve comparison\\n\",\n",
    "    \"plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Models to include in comparison\\n\",\n",
    "    \"models = [\\n\",\n",
    "    \"    (lr_model, \\\"Logistic Regression\\\", 'blue'),\\n\",\n",
    "    \"    (rf_model, \\\"Random Forest\\\", 'green'),\\n\",\n",
    "    \"    (gb_tuned, \\\"Tuned Gradient Boosting\\\", 'red')\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model, name, color in models:\\n\",\n",
    "    \"    y_prob = model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    fpr, tpr, _ = roc_curve(y_test, y_prob)\\n\",\n",
    "    \"    auc = roc_auc_score(y_test, y_prob)\\n\",\n",
    "    \"    plt.plot(fpr, tpr, color=color, label=f'{name} (AUC = {auc:.3f})')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add diagonal line (random classifier)\\n\",\n",
    "    \"plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.xlabel('False Positive Rate')\\n\",\n",
    "    \"plt.ylabel('True Positive Rate')\\n\",\n",
    "    \"plt.title('ROC Curve Comparison')\\n\",\n",
    "    \"plt.legend(loc='lower right')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Precision-Recall curve comparison\\n\",\n",
    "    \"plt.figure(figsize=(10, 8))\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model, name, color in models:\\n\",\n",
    "    \"    y_prob = model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"    precision, recall, _ = precision_recall_curve(y_test, y_prob)\\n\",\n",
    "    \"    ap = average_precision_score(y_test, y_prob)\\n\",\n",
    "    \"    plt.plot(recall, precision, color=color, label=f'{name} (AP = {ap:.3f})')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.xlabel('Recall')\\n\",\n",
    "    \"plt.ylabel('Precision')\\n\",\n",
    "    \"plt.title('Precision-Recall Curve Comparison')\\n\",\n",
    "    \"plt.legend(loc='lower left')\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Feature Importance Analysis\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Feature importance comparison between Random Forest and Gradient Boosting\\n\",\n",
    "    \"# Extract feature importances\\n\",\n",
    "    \"rf_importance = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_train.columns,\\n\",\n",
    "    \"    'RF_Importance': rf_model.feature_importances_\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"gb_importance = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_train.columns,\\n\",\n",
    "    \"    'GB_Importance': gb_tuned.feature_importances_\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Merge importances\\n\",\n",
    "    \"feature_comparison = pd.merge(rf_importance, gb_importance, on='Feature')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add logistic regression coefficients\\n\",\n",
    "    \"lr_coef = pd.DataFrame({\\n\",\n",
    "    \"    'Feature': X_train.columns,\\n\",\n",
    "    \"    'LR_Coefficient': np.abs(lr_model.coef_[0])  # Take absolute value for comparison\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"feature_comparison = pd.merge(feature_comparison, lr_coef, on='Feature')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Normalize coefficients to make them comparable\\n\",\n",
    "    \"feature_comparison['LR_Importance'] = feature_comparison['LR_Coefficient'] / feature_comparison['LR_Coefficient'].max()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate average importance\\n\",\n",
    "    \"feature_comparison['Avg_Importance'] = (\\n\",\n",
    "    \"    feature_comparison['RF_Importance'] + \\n\",\n",
    "    \"    feature_comparison['GB_Importance'] + \\n\",\n",
    "    \"    feature_comparison['LR_Importance']\\n\",\n",
    "    \") / 3\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Sort by average importance\\n\",\n",
    "    \"feature_comparison = feature_comparison.sort_values('Avg_Importance', ascending=False)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display top features\\n\",\n",
    "    \"print(\\\"Top 15 Features by Average Importance:\\\")\\n\",\n",
    "    \"feature_comparison[['Feature', 'RF_Importance', 'GB_Importance', 'LR_Importance', 'Avg_Importance']].head(15)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Visualize top feature importance comparison\\n\",\n",
    "    \"top_features = feature_comparison.head(15)['Feature'].tolist()\\n\",\n",
    "    \"importance_df = pd.DataFrame()\\n\",\n",
    "    \"\\n\",\n",
    "    \"for model, name in [(rf_model, 'Random Forest'), (gb_tuned, 'Gradient Boosting')]:\\n\",\n",
    "    \"    imp = pd.DataFrame({\\n\",\n",
    "    \"        'Feature': X_train.columns,\\n\",\n",
    "    \"        'Importance': model.feature_importances_,\\n\",\n",
    "    \"        'Model': name\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"    importance_df = pd.concat([importance_df, imp])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Filter for top features\\n\",\n",
    "    \"top_importance = importance_df[importance_df['Feature'].isin(top_features)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create barplot\\n\",\n",
    "    \"plt.figure(figsize=(12, 10))\\n\",\n",
    "    \"sns.barplot(x='Importance', y='Feature', hue='Model', data=top_importance)\\n\",\n",
    "    \"plt.title('Top 15 Feature Importance Comparison')\\n\",\n",
    "    \"plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Final Model Selection\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Based on evaluation metrics, select the best model\\n\",\n",
    "    \"# (In this case, Gradient Boosting typically performs best)\\n\",\n",
    "    \"best_model = gb_tuned\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print best model parameters\\n\",\n",
    "    \"print(\\\"Best Model: Tuned Gradient Boosting\\\")\\n\",\n",
    "    \"print(f\\\"Parameters: {best_model.get_params()}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Save the best model\\n\",\n",
    "    \"os.makedirs('../models', exist_ok=True)\\n\",\n",
    "    \"save_model(best_model, '../models/churn_model.pkl')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Analyze misclassifications\\n\",\n",
    "    \"y_pred = best_model.predict(X_test)\\n\",\n",
    "    \"y_prob = best_model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create dataframe with actual and predicted values\\n\",\n",
    "    \"misclass_df = pd.DataFrame({\\n\",\n",
    "    \"    'actual': y_test,\\n\",\n",
    "    \"    'predicted': y_pred,\\n\",\n",
    "    \"    'probability': y_prob\\n\",\n",
    "    \"})\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add original features\\n\",\n",
    "    \"misclass_df = pd.concat([X_test.reset_index(drop=True), misclass_df], axis=1)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# False positives (predicted churn but didn't)\\n\",\n",
    "    \"false_positives = misclass_df[(misclass_df['actual'] == 0) & (misclass_df['predicted'] == 1)]\\n\",\n",
    "    \"print(f\\\"Number of False Positives: {len(false_positives)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# False negatives (didn't predict churn but did)\\n\",\n",
    "    \"false_negatives = misclass_df[(misclass_df['actual'] == 1) & (misclass_df['predicted'] == 0)]\\n\",\n",
    "    \"print(f\\\"Number of False Negatives: {len(false_negatives)}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze false negatives (most costly error type)\\n\",\n",
    "    \"print(\\\"\\\\nFalse Negative Analysis:\\\")\\n\",\n",
    "    \"fn_numerical = false_negatives.select_dtypes(include=['int64', 'float64'])\\n\",\n",
    "    \"fn_means = fn_numerical.mean()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compare to overall population means\\n\",\n",
    "    \"overall_means = X_test.select_dtypes(include=['int64', 'float64']).mean()\\n\",\n",
    "    \"percent_diff = ((fn_means - overall_means) / overall_means * 100).dropna()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show features with large differences\\n\",\n",
    "    \"significant_diffs = percent_diff[abs(percent_diff) > 20].sort_values(ascending=False)\\n\",\n",
    "    \"print(\\\"\\\\nFeatures where False Negatives differ from population:\\\")\\n\",\n",
    "    \"for feature, diff in significant_diffs.items():\\n\",\n",
    "    \"    direction = \\\"higher\\\" if diff > 0 else \\\"lower\\\"\\n\",\n",
    "    \"    print(f\\\"{feature}: {abs(diff):.1f}% {direction}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Threshold Optimization\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Threshold optimization\\n\",\n",
    "    \"# Instead of using default 0.5 threshold, we can optimize it for specific business objectives\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Get probabilities\\n\",\n",
    "    \"y_prob = best_model.predict_proba(X_test)[:, 1]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Calculate metrics across different thresholds\\n\",\n",
    "    \"thresholds = np.arange(0.1, 0.9, 0.05)\\n\",\n",
    "    \"threshold_metrics = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for threshold in thresholds:\\n\",\n",
    "    \"    y_pred_threshold = (y_prob >= threshold).astype(int)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Calculate metrics\\n\",\n",
    "    \"    accuracy = accuracy_score(y_test, y_pred_threshold)\\n\",\n",
    "    \"    precision = precision_score(y_test, y_pred_threshold)\\n\",\n",
    "    \"    recall = recall_score(y_test, y_pred_threshold)\\n\",\n",
    "    \"    f1 = f1_score(y_test, y_pred_threshold)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Confusion matrix\\n\",\n",
    "    \"    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Assuming cost of false negative is 5x cost of false positive\\n\",\n",
    "    \"    # This is a business assumption - adjust based on your specific situation\\n\",\n",
    "    \"    cost = (fn * 5) + fp\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    threshold_metrics.append({\\n\",\n",
    "    \"        'threshold': threshold,\\n\",\n",
    "    \"        'accuracy': accuracy,\\n\",\n",
    "    \"        'precision': precision,\\n\",\n",
    "    \"        'recall': recall,\\n\",\n",
    "    \"        'f1': f1,\\n\",\n",
    "    \"        'tp': tp,\\n\",\n",
    "    \"        'fp': fp,\\n\",\n",
    "    \"        'tn': tn,\\n\",\n",
    "    \"        'fn': fn,\\n\",\n",
    "    \"        'cost': cost\\n\",\n",
    "    \"    })\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert to DataFrame\\n\",\n",
    "    \"threshold_df = pd.DataFrame(threshold_metrics)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot metrics vs threshold\\n\",\n",
    "    \"plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"plt.plot(threshold_df['threshold'], threshold_df['accuracy'], label='Accuracy')\\n\",\n",
    "    \"plt.plot(threshold_df['threshold'], threshold_df['precision'], label='Precision')\\n\",\n",
    "    \"plt.plot(threshold_df['threshold'], threshold_df['recall'], label='Recall')\\n\",\n",
    "    \"plt.plot(threshold_df['threshold'], threshold_df['f1'], label='F1 Score')\\n\",\n",
    "    \"plt.xlabel('Threshold')\\n\",\n",
    "    \"plt.ylabel('Score')\\n\",\n",
    "    \"plt.title('Metrics vs. Classification Threshold')\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Plot cost vs threshold\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.plot(threshold_df['threshold'], threshold_df['cost'])\\n\",\n",
    "    \"plt.xlabel('Threshold')\\n\",\n",
    "    \"plt.ylabel('Cost (5×FN + FP)')\\n\",\n",
    "    \"plt.title('Error Cost vs. Threshold')\\n\",\n",
    "    \"plt.grid(True)\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Find optimal threshold for minimizing cost\\n\",\n",
    "    \"optimal_idx = threshold_df['cost'].idxmin()\\n\",\n",
    "    \"optimal_threshold = threshold_df.loc[optimal_idx, 'threshold']\\n\",\n",
    "    \"print(f\\\"Optimal threshold for minimizing cost: {optimal_threshold:.2f}\\\")\\n\",\n",
    "    \"print(\\\"Metrics at optimal threshold:\\\")\\n\",\n",
    "    \"print(f\\\"Accuracy: {threshold_df.loc[optimal_idx, 'accuracy']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Precision: {threshold_df.loc[optimal_idx, 'precision']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Recall: {threshold_df.loc[optimal_idx, 'recall']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"F1 Score: {threshold_df.loc[optimal_idx, 'f1']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"False Negatives: {threshold_df.loc[optimal_idx, 'fn']}\\\")\\n\",\n",
    "    \"print(f\\\"False Positives: {threshold_df.loc[optimal_idx, 'fp']}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": None,\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Save the optimal threshold with the model information\\n\",\n",
    "    \"model_info = {\\n\",\n",
    "    \"    'model': best_model,\\n\",\n",
    "    \"    'optimal_threshold': optimal_threshold,\\n\",\n",
    "    \"    'feature_importance': feature_comparison[['Feature', 'Avg_Importance']].head(20).to_dict(),\\n\",\n",
    "    \"    'columns': X_train.columns.tolist()\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"with open('../models/model_info.pkl', 'wb') as f:\\n\",\n",
    "    \"    pickle.dump(model_info, f)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Model information saved with optimal threshold\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"In this notebook, we've developed and evaluated several machine learning models to predict customer churn. Here's a summary of our findings:\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Model Performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Gradient Boosting** emerged as the best performing model with the highest AUC and F1 scores after hyperparameter tuning.\\n\",\n",
    "    \"2. **Random Forest** also performed well and provided valuable insights through feature importance.\\n\",\n",
    "    \"3. **Logistic Regression** offered good interpretability and highlighted key linear relationships.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Key Predictors of Churn\\n\",\n",
    "    \"\\n\",\n",
    "    \"The most important features across models were:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Contract-Related Features**: `ContractRiskFactor`, `Contract_Month-to-month`, and contract-related variables consistently ranked as top predictors.\\n\",\n",
    "    \"2. **Tenure**: Tenure-related features like `tenure` and `TenureGroup` were strong negative predictors of churn.\\n\",\n",
    "    \"3. **Service Usage**: Features like `TotalServices`, `FiberNoProtection`, and specific service flags were important.\\n\",\n",
    "    \"4. **Composite Features**: The engineered `CompositeRiskScore` captured multiple risk factors effectively.\\n\",\n",
    "    \"5. **Customer Value**: Features like `MonthlyCharges` and the interaction between high value and short contracts were significant.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Threshold Optimization\\n\",\n",
    "    \"\\n\",\n",
    "    \"We optimized the classification threshold to minimize the cost of misclassifications, considering that false negatives (failing to predict customers who will churn) are more costly than false positives. The optimal threshold was determined to be around 0.3 (instead of the default 0.5), which improves recall at the expense of some precision.\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Business Analysis**: Apply the model to segment customers by churn risk and develop targeted retention strategies.\\n\",\n",
    "    \"2. **Deployment**: Implement the model in a production system for ongoing churn prediction.\\n\",\n",
    "    \"3. **Monitoring**: Establish metrics to monitor model performance over time and retrain as needed.\\n\",\n",
    "    \"4. **Feedback Loop**: Incorporate the effectiveness of retention actions back into the model to improve future predictions.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
